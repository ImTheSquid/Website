---
title: The Art of Failure
description: Reflections on my undergraduate research and my relationship with it over the past three years.
tags: [programming, research, life]
---

What happens when the one thing you've poured all your energy into over multiple years doesn't ship? A few years ago, I came to Purdue looking to learn and make an impact. In some ways, I have certainly succeeded: my incredible friends, strong academics, transformative [study abroad experience](/blog/oh-the-places-youll-go), and steadfast devotion to [Purdue Hackers](https://purduehackers.com) have completely changed the course of my life for the better. However, not everything can always work out perfectly. This post is a story about CodeCheck, the research I worked on for the past three years. Specifically, this is my place to reflect on what CodeCheck was, what the process of developing it taught me, and how the lack of concrete results has taught me to find the beauty and meaning in the process of making things and use that meaning to move forward stronger than I started.

# Conception

CodeCheck was born completely by chance. One day I received an email from Professor Andres Bejarano asking for researchers to tackle an interesting problem: source code plagiarism. I was intrigued and had been thinking about research a lot recently, so I decided to go to the callout. When I got there I clicked with Andres very quickly and I became hooked on the idea. Some other interested students and I coalesced into a group and we got to work. I quickly took a leading role in the group, managing task delegation and nudging towards certain areas of research.

## The Problem

It’s no surprise that students tend to cheat off of each other on difficult assignments, especially before the advent of generative AI assistants like ChatGPT. This problem was especially common in computer science classes where usually the only objective is to pass a certain test suite by any means necessary. I’ve heard of particularly creative methods of achieving this including but not limited to fuzzing and decompiling the test executable; however, copying a friend’s code isn’t creative and doesn’t teach you anything useful.

Unsurprisingly, Purdue and many other schools try to clamp down as hard as they can on plagiarism, leading to tools like [JPLAG](https://github.com/jplag/JPlag) and [MOSS](https://theory.stanford.edu/~aiken/moss/). While these applications do a decent job, a whole ecosystem of tips and tricks have developed to fool these tools. The big problem comes down to the fact that all tools currently in use by universities rely on direct string comparisons between samples, meaning they take the literal syntax and directives that students wrote and do something only slightly more complex than checking the differences between words. In addition, other vulnerabilities exist, such as MOSS being blind to spacing differences, that make it trivial to get around plagiarism detection.

To pinpoint the problem: text is easy to deal with; it’s malleable and humans (and generative AI) intuitively understand how to manipulate it. When perturbed or obfuscated text is passed to these deterministic algorithms, they fail to detect the underlying plagiarism still present in the text. This issue has become especially severe with generative AI available to everyone, ready to rewrite in a way that’s just good enough to fool detectors.

> Note: I made the assumption that most large language models (LLMs) would just rename variables and do trivial reordering that didn’t affect the underlying logic of the code, an assertion which was true when LLMs first arrived in the hands of students. This assumption would weaken and eventually become invalid over the course of the following years and many large leaps in LLM capabilities, but I chose to ignore this decreasing effectiveness for a very long time to get a chance to dive deep into the problem at hand.

## The Solution

The intuition for my proposed solution is quite simple: what do computers do when they want to get rid of all the bullshit that comes with plaintext? The answer is to convert it to a data structure that abstracts away all of the characters into a common syntax, organized in a tree-like structure to, in the case of compilers, make code generation much easier. You may be able to see where I’m going with this: abstract syntax trees (ASTs).

(INSERT TYPST REPRESENTATION HERE)

ASTs were perfect! They removed all of the nonsense from plaintext, creating a pure representation to operate on. Further refining the output I was looking for, I would want pairs of line spans $((l^1_1, l^1_2), (l^2_1, l^2_2))$ derived from the tree structure such that the contents were plagiarized from each other.

# Honeymoon

My group made consistent progress throughout the first year and a half, from fall 2022 to spring 2023. We had regular meetings both amongst ourselves and with Andres, we were consistently coming up with new ideas, and it looked at the time like we would have a prototype ready for presentation very soon. In fact, this dream seemed to turn into reality when we came across tree kernels, a way of determining the similarity index across trees. I was reading through the paper and I thought it would be pretty easy to make a good implementation of the concept, but I managed to miss the implications of this important equation:

$$
K_{tree}(T_1, T_2) = {V_{T_1}}^T V_{T_2} = \sum_{n_1 \in N_{T_1}} \sum_{n_2 \in N_{T_2}} C(n_1, n_2)
$$

Where $V_{T_i}$ is a vector of all possible subtrees of tree $i$ and $C$ is a recursive comparison function in $O(n^x)$[^1]. This equation defines the search protocol for the tree kernel, and would end up sinking this entire branch of research. To explain why, let’s take a closer look at this equation. The most relevant portion is the recursive condition of $C$, defined as:

$$
C(n_1, n_2) = \lambda_{tree} \prod_i^{\textrm{nc}(n_1)} (1 + \max_{\textrm{ch} \in \textrm{ch}_{n_2}} C(\textrm{ch}_i(n_1), \textrm{ch}))
$$

Where $\lambda_{tree}$ is a decay factor, $\textrm{nc}$ is the number of children function, and $\textrm{ch}_i$ is a set of child nodes. This product of sums combined with a product over a recursive set took forever with a naive implementation even on medium-sized collections ($n \approx 200$) of small length programs ($l \leq 50$). After considering this equation, I had two options: try to optimize it enough that it would work, or give up on it and search elsewhere.

With this computational explosion, I realized we needed to pivot to an alternative method for finding plagiarism. After much debate we settled on a deep neural network as the most promising direction of study and started doing research on potential model architectures. However, as time passed, many of the members of the group ended up dropping off due to other commitments or increasing class loads, eventually leaving only myself to continue.

# The Lonely Path Forward

Fast-forward a year and a half, and I’m still trying my hardest to push CodeCheck forward while balancing my own commitments including an increasingly intense class load, internships during the summer, social obligations, and membership in clubs including Purdue Hackers; it’s not easy to do and I am going a lot slower than I’d like to, but I was still doing research. Andres was incredibly supportive of me during this time and I’ll always be thankful for his consistency when I was hitting dead end after dead end.

After learning more about the architecture of LLMs I decided to try using a transformer to solve the problem. I did some trial implementations and eventually got a model working. Around this time Andres told me I could consult with an AI professor on campus, something I took advantage of immediately, and good thing I did! He promptly tore my implementation to shreds and showed me why it wouldn’t work: transformers have no idea of “numbers” or “spans”, and therefore even if they could tell if two blocks of code were plagiarized, it would have no idea how to report that information back to the overseer with any real reliability. For a real-world analogy, this is why LLMs have so much trouble counting the letters in words or doing numeric math problems: numbers are not something they can “understand”.

Coming out of this meeting I was slightly dejected but also relieved that I could cut off a dead end. I had to do some soul searching in order to find my next architecture, but eventually I had a realization: why abandon the AST structure for the text contained within it, needlessly discarding essential position information encoded into the structure? This epiphany led me down a rabbit hole to discover graph neural networks and a sub-subject, graph attention networks.

## Graph Attention Networks: The Best Thing Since Sliced Bread

Graph attention networks, or GATs, are really cool and were honestly the perfect solution to my architecture. Consider the transformer: it operates on each token, attending to every other token in the sequence to change the embedding of said token to better match the context around it.

Graph attention networks do a similar operation, changing the embedding of the node (and optionally edges) to more closely match the inferred meaning from the rest of the graph.

There are many different variations of GATs that use different rules for when and how to update each node with each other node, but the general underlying principle of having other nodes tell a target node how it should update its embedding is the core of it. This was perfect for CodeCheck: I could retain the structural information of the tree (specifically the line numbers of each token) as metadata while allowing the model to operate on the embeddings of each node, not even knowing of the existence of line numbers.

I felt great at this point, but I still needed to define a model around this base component, a task that proved much more difficult than I imagined. The biggest problem was figuring out how to structure the loss of the model. Making a model “find plagiarism” is a multi-layered task including, but not limited to:

- Deciding what “plagiarism” entails given parameters including task complexity, code styling, assignment requirements, and administrator wishes, then correctly implementing these requirements in a model that cannot easily be changed once it’s trained.
- Finding instances of plagiarism across thousands of source files and correctly linking two (or more!) files as containing plagiarized code.
- Correctly identifying the exact line spans (of which there can be multiple!) to tag as plagiarism with a razor-thin margin of error.

Suffice to say, this wouldn’t be something I could just encode in a single simple loss function. I originally started with a regressive model utilizing distance intersection over union (DIoU), a function used by object classification models. I tried to find new and innovative ways to shove all of the objectives I wanted to track into a single function, but the model never seemed to be able to handle it. This function needed to be able to either immediately reduce the size of the AST to its relevant parts or do so in iterative steps, both of which I failed to accomplish with a regressive model. After much trial and mostly error I decided to pivot to reinforcement learning.

## Reinforcement Learning

For those who don’t know, reinforcement learning (RL) is a subfield of machine learning where a policy, a decision-making model, takes input from the environment and turns it into an action that is then imparted upon the environment. This action can also be graded by a reward signal, an additional data point used to improve the policy, where higher rewards are promoted.

(RL diagram)

CodeCheck benefits from the more abstracted reward signal that can encode the many factors I wanted to consider as well as the iterative nature of the changing environment. As the tree is pruned, the problem space gets smaller and different nodes need to be removed. I chose RL to try to make a long-term policy that can successfully decide iteratively which nodes to prune. In hindsight something like diffusion or flow models may have also worked, but I was interested in RL not just because of its potential application to CodeCheck but also its uses in robotics and other fields and I wanted an excuse to learn it.

I ended up with an advantage actor critic (A2C), where an actor model executes a policy on the data and the critic model grades its performance with a metric called advantage, the perceived value of taking the action the actor did above the average value of the state. A negative advantage means the action taken was worse than average, a positive advantage means the action taken was better. I trained the actor to cut away certain parts of the tree, and the value function to assign rewards based on multiple metrics including code size, relevance, and interpretability. This would all be factored into a single Q function $Q(s, a)$ and compared to the value function for the state $V(s)$.

I spent months shaping the reward and tuning other hyperparameters attempting to get my model to perform, but I consistently ran into extreme underfitting. To attempt to fix my model I did two things, one of which I think was smart and the other not so much.

### Band-Aid 1: A Custom Embedding Model

I realized that trying to shove my trees straight into the main model may lead to issues with data sparsity, since originally I was just feeding the vectors straight in as one-hot encodings. To remedy this I created a small embedding model with what I think was a pretty smart objective: do triplet loss on the embedding of the text description of each node in the language’s grammar. For example, if I had a node $x$ from C with label `IfStatementCondition`, I would try to bring it closer to a similar node $x'$ from C++ with label `IfStatement` and farther from a node $z$ with label `VariableDeclaration`. I also had auxiliary objects to keep nodes near their own language and to try to guess the next node in a preorder traversal to get some understanding of the tree itself.

I was quite proud of this embedding model, and it certainly did *something*, but unfortunately it was not enough to get the main model to gain an understanding of the task at hand. It’s of course possible I just needed to tune more hyperparameters, but I was a one-man show and starting to run thin on patience.

### Band-Aid 2: Synthetic Data

Unsurprisingly people tend to not be very forthcoming about copying off of others in an academic setting, and said cheaters are not likely to donate their code and the code of their accomplice to open datasets I can use to train off of. This data sparsity led me to search the darkest corners of the internet for any possible datasets I could find, without much luck. After  a long string of disappointing datasets I decided to try a strategy I had avoided for the longest time: synthetic data.

After all of the discourse on [model collapse](https://arxiv.org/pdf/2410.04840) I had wanted to avoid using synthetic data at all costs. I even went so far as to design my own labelling suite to try to make it easier on myself and my team to label our own data, but due to the sparsity of any coding assignments at all, that initiative failed. I found myself trudging back to LLMs and hoping I wouldn’t fall victim to a degradation loop. The good news is I didn’t have any issues with the quality of data generated by the LLMs I used. The bad news is even with the power of the entire internet, LLMs were unable to generate enough high-quality data for me to use in CodeCheck.

LLMs failed on many measures, but their two biggest drawbacks were inconsistency and cost. LLMs are great when you want them to write an email for you, but terrible when you want consistently-formatted JSON for a problem that fundamentally requires some creativity and cross-problem application. After many iterations of prompt tuning I was only able to get an around 70% success rate at generating samples. Even achieving that relatively low success rate was a very interesting challenge that I will touch on now.

### Aside: Bending LLMs to Your Will

I like to think of LLMs as occasionally malicious toddlers that generally want to do exactly what they want, not what you want. After learning my lesson trying to shove line numbers directly into my own transformer in the first version of CodeCheck, I realized I needed to keep numbers as far removed from the LLM as possible. To do this, I instead leaned on a markup language many LLMs know as second nature: HTML. I would instruct the LLM to generate HTML tag pairs `<plag IDENTIFIER>CODE</plag>` to wrap the code the LLM decided was plagiarism. To demonstrate, below is an example of two generated snippets of “plagiarized” code:

```py
def sort_numbers(arr: list[int]):
	for i in range(1, len(arr)):
        key = arr[i]
        j = i - 1

		<plag while>
        while j >= 0 and key < arr[j]:
            arr[j + 1] = arr[j]
            j -= 1
        arr[j + 1] = key
        </plag>
```

```java
void sort(int arr[]) {
    int n = arr.length;
    for (int i = 1; i < n; ++i) {
        int key = arr[i];
        int j = i - 1;

		<plag while>
        while (j >= 0 && arr[j] > key) {
            arr[j + 1] = arr[j];
            j = j - 1;
        }
        arr[j + 1] = key;
		</plag>
    }
}
```

This method, designed to look and act somewhat like tool calls, was tailored to allow LLMs to say where the plagiarism was without needing to give potentially faulty line numbers. When the LLM managed to generate correct tag pairs, a common point of failure, I had parsing code that would go through and remove the tags and replace them with the actual line numbers. This system, along with other failsafes like a very detailed prompt and error checking on opening and closing tags, allowed me to achieve much more reliable data generation.

However, even with all of these measures, cost crept in as the silent killer. I am fortunate to have a Mac that can run some of the best LLMs completely on-device, but at the scale I needed the tokens per second was far too low; I would need gigabytes if not terabytes of data, and I didn’t have infinite time. This problem led me to rent GPUs from RunPod, which of course has actual costs associated with it instead of me being able to leech off of Purdue’s power with my Mac. RunPod had much better token per second performance, at the cost of draining my bank account very, very fast. In the end I realized that even with the speed I was going at and using faster LLMs, this just wasn’t going to work.

# The Long Tail

Many different events in my life led to me deciding to close the book on CodeCheck. In August I took a trip to San Francisco to visit friends and by chance ran into the co-founder of Era Labs, Liz. I brought up a project I had envisioned called Beacons and we hit it off, with me eventually joining Era Labs as a fellow. This newfound position gave me a project I was really excited to work on and greatly reduced my already dwindling interest in working on CodeCheck. I’ve realized that I do much better with projects when I have small, consistent ships reinforcing my belief that I’m making something awesome; CodeCheck lacked these entirely. I felt like Sisyphus pushing his boulder, constantly being thrown back down to the valley by yet another stupid problem, and you may see why I lost hope in CodeCheck.

Around this time graduate school became a real thing I needed to think about. I realized I had a choice: either continue trying to grind CodeCheck and hope I could miraculously get insane results and somehow convert it to a presentable state, or focus on researching fields of interest and professors who would bring me to the next level of understanding in what I care about. I am not usually one for giving up on projects I care about, but with all that was happening and the pressure to go through another application cycle, the scales tilted in favor of cutting off CodeCheck.

Of course there’s also the elephant in the room: much-improved LLMs. By the time I had gotten to this point in September 2025, LLMs had become scarily good at writing code and very few of them had any problems helping with plagiarism. There’s a difference between trying to match between students within a few sections and potentially a few GitHub repositories and the entirety of human knowledge! I did have some ideas to fix this problem such as incorporating some LLM-generated outputs given the assignment details as “students”, but after dealing with such a data desert I had no willpower to continue to try to fight LLMs.

All of these things had various levels of impact on my decision, but the biggest problem that wouldn’t get out of my head was the overwhelming complexity of the problem and the extreme lack of data. If either of those problems didn’t exist I may have been able to push forward, but unfortunately plagiarism is an inherently subjective topic and I am currently bound by the laws of the universe to have limited time to spend on generating potentially useless synthetic data. That’s not to say this problem is impossible to solve with current methods, quite the opposite! I firmly believe that if someone devoted their masters thesis or PhD dissertation to the problem it would likely be possible to create a system like the one I envisioned. My friend John has, throughout the years, proposed many interesting areas of research I didn’t have time to explore such as alternate transformer models, RL methods, fine-tuning other models, and data aggregation architectures that may be the key to finally unblock me. Alas, other more interesting problems have now fully captivated me and after writing this post I will be satisfied with the work I’ve done.

---

The story of CodeCheck may be over, but I will never forget the ways it changed me and helped to mold me into who I am now.

# Venturing to the Ends of the Earth

I would consider myself to be a pretty rabbit hole-prone person;  if I find an interesting thread, I will more than likely pull it until I find myself buried under a mountain of twine. CodeCheck hijacked this instinct and put it into overdrive. I found myself going through so many papers I couldn’t even keep up with my own mind at some points. This obsession didn’t even cover the many, many hours I spent scouring the internet for plagiarism datasets, only to find none. Even then, I became the world’s best detective for plagiarized code, and I’m happy I did.

Rarely do I become so singularly dedicated to a single project, but CodeCheck showed what could happen when I let it become my sole star. Seeing myself go to all these lengths makes me very excited for whatever rabbit hole next catches my eye, since I know I will take the time to become intimately familiar with the subject matter and I can trust myself to check every last crevice to find the best possible solution. If there’s a very important lesson I learned, it’s choosing what objectives and subjects to peruse and which to leave until later. Structured information is the best possible fuel for me to make insights and along with my reluctance to give up, it’s a potent combination.

# CodeCheck, the Catalyst

CodeCheck has, for better and worse, affected me in so many different ways. Before I started CodeCheck I knew I wanted to go into machine learning, but I had pretty much no idea how any part of the math worked or which subfield I wanted to explore. As CodeCheck progressed I was forced to confront these questions. I slowly became very partial to RL and specifically hierarchical reinforcement learning, where multiple policies work together in a delegation fashion to achieve complex tasks by distributing the knowledge for doing so across different levels of complexity. I was also able to get into a bit of graph theory and graph neural networks, two topics I was vaguely aware of starting CodeCheck but now I find to be very interesting topics.

The impact CodeCheck has had isn’t limited to my academic pursuits. One day Matthew, the previous president of Purdue Hackers, invited me to a CS partners lunch where clubs would go and chat with companies regarding sponsorships. By chance I happened to sit across from a PhD in computer science who works at Peraton Labs. We hit it off talking about our research experiences, one thing led to another, and I got an internship at Peraton Labs over the summer! When I first started at Labs I was placed on the networking team; said team had some amazing people working on it but I knew deep down I wasn’t super interested in the subject matter. However, one day I overheard some of my co-interns discussing a graph neural network problem.  I asked them to have a meeting with their supervisor, and after using my knowledge of graph neural networks gained through CodeCheck I got on the team! I then spent the rest of the summer working on something very exciting, and I wouldn’t have gotten the opportunity to do so without CodeCheck.

There were of course days where I felt down, even hopeless, at the thought of trying to continue the project, but when I would eventually push through and continue working my resolve was strengthened beyond my wildest dreams. I learn by failure, and the setbacks I encountered while designing CodeCheck’s architecture taught me more about how to design machine learning architectures taught me more than any class I took during my undergrad. I will take the skills and knowledge I gained forward with me to wherever I go next, ready to climb my next hill with all the strength I’ve got.

# Build, Fail, then Build Again

I am endlessly thankful to Andres and everyone else who supported me while working on CodeCheck and excited to work on whatever’s next. CodeCheck may have been a failure in some respects, but in others it was a work of art. The intricacies of building a complex model to find plagiarism created something nonfunctional in this specific case but still beautiful nonetheless. Every step was dutifully thought out by myself and others, and the journey mattered much more than the destination. I’m reminded of a quote by Larry Page: “It’s very hard to fail completely if you aim high enough”, and I don’t think there’s a word to describe how high I aimed; shooting for the moon would be an understatement.

CodeCheck was yet another entry in my endless cycle of building. I ideate, I create, then I either ship or learn what to do better next time. I tend to oscillate interest between software and hardware projects, so I’ve really enjoyed working on Beacons since they give me that physical presence CodeCheck never would’ve had even if it had worked out perfectly. I don’t know where I’ll be or what I’ll build next, but I’m excited for whatever the world sends my way. Anything and everything can contribute to the art I make.

[^1]: Song, H.-J., Park, S.-B., and Park, S. Y. Computation of program source code similarity by composition of parse tree and call graph. Mathematical Problems in Engineering, 2015:429807, 2015. doi:10.1155/2015/429807.
